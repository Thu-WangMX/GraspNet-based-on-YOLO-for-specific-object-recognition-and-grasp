# ğŸ¤– GraspNet-based-on-YOLO-for-specific-object-recognition-and-grasp

While GraspNet can generate a large number of object grasping poses, it does not explicitly select the object to be grasped as the grasping target. Instead, it generates grasping poses for all objects within the field of view, making it unsuitable for grasping specific objects.
So we provide this, a small research project that connects **YOLOv8**, **GraspNet baseline** and a **Flexiv robot** for stain / objectâ€“aware grasping on a tabletop.

---

## âœ¨ What it does

- ğŸ§  **YOLOv8** detects target stains / objects on RGB images and generates a workspace mask.
- ğŸ“· **RealSense RGBâ€‘D** provides depth, used to build a masked point cloud.
- âœ‹ **GraspNet** predicts 6â€‘DoF grasp poses on the masked point cloud.
- ğŸ¦¾ **Flexiv** executes the selected grasp (approach â†’ close gripper â†’ lift â†’ place).

---

## ğŸ“‚ Key files

- `grasp_out_api.py` â€“ main endâ€‘toâ€‘end demo (YOLO + GraspNet + Flexiv).
- `robot-stain-perception/tools/grasp_mask_api.py` â€“ YOLO + depth â†’ mask.
- `robot-stain-perception/tools/realsense_api.py` â€“ RealSense wrapper (frames + intrinsics).
- `FlexivRobot.py` & `trajectory_planner.py` â€“ robot control helpers.
- `pointnet2/` â€“ PointNet++ CUDA ops for GraspNet.

---

## âš™ï¸ Setup (minimal)

```bash
conda create -n flexiv python=3.10 -y
conda activate flexiv

# Example (CUDA 11.7, adjust if needed)
pip install torch==1.13.1 torchvision==0.14.1 torchaudio==0.13.1 \
  --index-url https://download.pytorch.org/whl/cu117

pip install opencv-python pyrealsense2 open3d pyyaml tqdm ultralytics flexivrdk

cd pointnet2 && python setup.py install && cd ..
```

Place your models:

- ğŸ§  YOLO weights, e.g. `yolo8l_batch8_run1.pt` in the repo root.
- âœ‹ GraspNet checkpoint in `checkpoints/`, e.g. `checkpoints/checkpoint-rs.tar`.

---

## â–¶ï¸ Run the demo

> âš ï¸ Make sure the Flexiv robot is in a safe and clear workspace.

```bash
conda activate flexiv
python grasp_out_api.py
```

The script will:
1. Capture RGBâ€‘D from RealSense.
2. Detect stains / objects and save a binary mask in `generated_masks/`.
3. Predict grasp poses with GraspNet.
4. Ask for confirmation and command the Flexiv arm to execute the grasp.

---

## ğŸ“¸ Qualitative Results on Toilet Trash

| åƒåœ¾ç±»åˆ«                   | GraspNet é¢„æµ‹æŠ“å–ç¤ºä¾‹                     |
|---------------------------|------------------------------------------|
| tissue_small                  | ![Small](docs/grasps/tissue_small.png)  |
| tissue_thin_2_5cm   | ![Thin2.5](docs/grasps/tissue_thin_2_5cm.png) |
| tissue_thin_1_0cm   | ![Thin1.0](docs/grasps/tissue_thin_1_0cm.png) |
| tissue_large_4sheet          | ![4sheet](docs/grasps/tissue_large_4sheet.png)   |
| sanitary_pad_ball            | ![Pad](docs/grasps/sanitary_pad_ball.png) |
| tissue_wet                      | ![Wipe](docs/grasps/wet_wipe.png)       |
| cigarette                      | ![Cigarette](docs/grasps/cigarette.png) |

## ğŸ™ Acknowledgements

This repo builds on:

- GraspNet Baseline
- YOLOv8 by Ultralytics
- Flexiv RDK
- Intel RealSense SDK
